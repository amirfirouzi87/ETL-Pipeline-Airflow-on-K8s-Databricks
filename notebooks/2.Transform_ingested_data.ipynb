{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f1546a3-e1cc-4853-ae4b-a9ca04df9b9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_raw = spark.readStream.table(\"yt_vids_data.raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dcccaa71-eed3-40b9-9c50-63fc355de939",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "df_cleaned = df_raw.select(\n",
    "    col(\"video_id\"),\n",
    "    col(\"title\"),\n",
    "    col(\"published_at\").cast(\"timestamp\"), # Spark handles ISO string to Timestamp well\n",
    "    col(\"duration\"),\n",
    "    col(\"view_count\").cast(\"long\"),        # \"47341951\" -> 47341951L\n",
    "    col(\"like_count\").cast(\"long\"),\n",
    "    col(\"comment_count\").cast(\"long\"),\n",
    "    col(\"created_date\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3164e2ea-57cc-4ba1-a4a2-85dfa5e5e368",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, regexp_extract, coalesce, lit, format_string, nullif, current_timestamp\n",
    "\n",
    "# 1. Extract D, H, M, S\n",
    "# We look for digits preceding the specific letter. \n",
    "# 'D' is unique to Days. \n",
    "d = coalesce(nullif(regexp_extract(col(\"duration\"), r'(\\d+)D', 1), lit('')).cast(\"int\"), lit(0))\n",
    "h = coalesce(nullif(regexp_extract(col(\"duration\"), r'(\\d+)H', 1), lit('')).cast(\"int\"), lit(0))\n",
    "m = coalesce(nullif(regexp_extract(col(\"duration\"), r'(\\d+)M', 1), lit('')).cast(\"int\"), lit(0))\n",
    "s = coalesce(nullif(regexp_extract(col(\"duration\"), r'(\\d+)S', 1), lit('')).cast(\"int\"), lit(0))\n",
    "\n",
    "# 2. Calculate total seconds\n",
    "# Add (d * 86400) for the days component\n",
    "total_seconds_col = (d * 86400) + (h * 3600) + (m * 60) + s\n",
    "\n",
    "# 3. Calculate normalized Days, Hours, Minutes, Seconds from the total\n",
    "final_d = (total_seconds_col / 86400).cast(\"int\")\n",
    "final_h = ((total_seconds_col % 86400) / 3600).cast(\"int\")\n",
    "final_m = ((total_seconds_col % 3600) / 60).cast(\"int\")\n",
    "final_s = (total_seconds_col % 60).cast(\"int\")\n",
    "\n",
    "# Calculate Total Hours (including days)\n",
    "total_hours = (total_seconds_col / 3600).cast(\"int\")\n",
    "final_m = ((total_seconds_col % 3600) / 60).cast(\"int\")\n",
    "final_s = (total_seconds_col % 60).cast(\"int\")\n",
    "\n",
    "# Format: 50:30:10\n",
    "df_total_hours = df_cleaned.withColumn(\n",
    "    \"duration_formatted\", \n",
    "    format_string(\"%02d:%02d:%02d\", total_hours, final_m, final_s)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78faf079-2190-4bc5-ae03-f5f3e6c9f381",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_selected = df_total_hours.select(\n",
    "    col(\"video_id\"),\n",
    "    col(\"title\").alias(\"Video_Title\"),\n",
    "    col(\"published_at\").cast(\"date\").alias(\"Published_Date\"),\n",
    "    col(\"view_count\").alias(\"View_Count\"),\n",
    "    col(\"like_count\").alias(\"Like_Count\"),\n",
    "    col(\"comment_count\").alias(\"Comment_Count\"),\n",
    "    col(\"duration_formatted\").alias(\"Duration\"),\n",
    "    col(\"created_date\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "737b3a6a-b89b-4a86-ad4c-0de922fe16cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "  # 1. Database creation (This stays the same)\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS yt_vids_data\")\n",
    "\n",
    "# 2. Define Paths \n",
    "# (Make sure the checkpoint path is in your actual storage account, not the placeholder I gave earlier)\n",
    "save_path = \"abfss://processed@youtubevideosmetadata951.dfs.core.windows.net/yt_vids_data\"\n",
    "processed_checkpoint = \"abfss://processed@youtubevideosmetadata951.dfs.core.windows.net/checkpoints/silver_vids\"\n",
    "\n",
    "# 3. Write Stream\n",
    "(df_selected.writeStream\n",
    "  .format(\"delta\")\n",
    "  .outputMode(\"append\")                        # CHANGED: Use outputMode(\"append\") instead of mode(\"overwrite\")\n",
    "  .option(\"checkpointLocation\", processed_checkpoint) # ADDED: Mandatory for streaming\n",
    "  .option(\"path\", save_path)                   # External Table location\n",
    "  .trigger(availableNow=True)                  # ADDED: Process new data only, then stop\n",
    "  .toTable(\"yt_vids_data.processed\")           # CHANGED: Use .toTable() instead of .saveAsTable()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12782f20-60fd-458d-9c8e-061e365e3819",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from yt_vids_data.processed\n",
    "where video_id = 'yERIXEwAQjU'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7098e539-ebce-40ab-883d-fd59b666b4e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7017206424711044,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "2.Transform_ingested_data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
